{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import unidecode\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from typing import Callable\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Projects\\\\text-classification'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the current working directory to the pachage root\n",
    "# That's step is due to the way settings.py is defined\n",
    "root_path_list = os.getcwd().split(\"\\\\\")[:-1]\n",
    "root_path = os.path.join(root_path_list[0], os.sep, *root_path_list[1:])\n",
    "os.chdir(root_path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User parameters\n",
    "\n",
    "data_folder = os.path.join(os.getcwd(), \"data\", \"01_raw\")\n",
    "data_file = \"complaints.csv\"\n",
    "\n",
    "test_size=0.2\n",
    "seed=0\n",
    "\n",
    "max_num_samples_per_class = 500 # 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>purchase order day shipping amount receive pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>forwarded message date tue subject please inve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>retail_banking</td>\n",
       "      <td>forwarded message cc sent friday pdt subject f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>credit_reporting</td>\n",
       "      <td>payment history missing credit report speciali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>credit_reporting</td>\n",
       "      <td>payment history missing credit report made mis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id          category                                          narrative\n",
       "0   0       credit_card  purchase order day shipping amount receive pro...\n",
       "1   1       credit_card  forwarded message date tue subject please inve...\n",
       "2   2    retail_banking  forwarded message cc sent friday pdt subject f...\n",
       "3   3  credit_reporting  payment history missing credit report speciali...\n",
       "4   4  credit_reporting  payment history missing credit report made mis..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "\n",
    "data_path = os.path.join(data_folder, data_file)\n",
    "data = pd.read_csv(data_path, header=0, sep=',', quotechar='\"')\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 162411 entries, 0 to 162420\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   id         162411 non-null  int64 \n",
      " 1   category   162411 non-null  object\n",
      " 2   narrative  162411 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 5.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>num_complaints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>retail_banking</td>\n",
       "      <td>13535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>credit_card</td>\n",
       "      <td>15566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mortgages_and_loans</td>\n",
       "      <td>18990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>debt_collection</td>\n",
       "      <td>23148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>credit_reporting</td>\n",
       "      <td>91172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              category  num_complaints\n",
       "4       retail_banking           13535\n",
       "0          credit_card           15566\n",
       "3  mortgages_and_loans           18990\n",
       "2      debt_collection           23148\n",
       "1     credit_reporting           91172"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('category').agg(num_complaints=('id','count')).reset_index().sort_values(by='num_complaints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forwarded message date tue subject please investigate comenity bank retailer card scam sent hello name scammed comenity bank credit card provider company childrens place new york forever victoria secret original credit comenity bank lower limit began charge overage fee along late fee began pay close attention card find limit also changed well incurring overage late fee reached company comenity bank stated would change credit limit original limit reached told summit payment account corrected comenity bank credit card impacted credit score plummeted negative status im currently paying price due corruption affected detrimental way debt due company charging overage fee well late fee even initial credit limit fluctuating tremendously company charge major fee account willing correct account nervous said attorney reason im reaching im employee company ruining credit plz help name contact info thank\n"
     ]
    }
   ],
   "source": [
    "print(data.narrative[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into X and y\n",
    "X = np.array(data.narrative)\n",
    "y = np.array(data.category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "\n",
    "def text_cleaner(X, min_len_clean_sentence=5):\n",
    "\n",
    "  for i in range(len(X)):\n",
    "    sentence = X[i]\n",
    "    sentence = sentence.lower() # lowercase text\n",
    "    sentence = unidecode.unidecode(sentence) # remove accents\n",
    "    sentence = re.sub(r\"\\W\",\" \",sentence) # remove non letters and numbers\n",
    "    sentence = re.sub(r\"\\d\",\" \",sentence) # remove numbers\n",
    "    sentence = re.sub(r\"\\b[a-z]{1}\\b\",\" \",sentence,flags=re.I) # remove word size 1\n",
    "    sentence = re.sub(r\"\\b[a-z]{2}\\b\",\" \",sentence,flags=re.I) # remove word size 2\n",
    "    sentence = re.sub(r\"\\s+\",\" \",sentence) # remove adittional space between words\n",
    "    sentence = re.sub(r\"^\\s+\",\"\",sentence) # remove adittional space in the begining\n",
    "    sentence = re.sub(r\"\\s+$\",\"\",sentence) # remove adittional space in the end\n",
    "    X[i] = sentence\n",
    "  return X\n",
    "\n",
    "X_clean = text_cleaner(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Filtering\n",
    "\n",
    "# Filter out samples without a minimum number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 4, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taget Label Encoding\n",
    "\n",
    "def label_encoder(x):\n",
    "    encoder = LabelEncoder()\n",
    "    x_encoded = encoder.fit_transform(x)\n",
    "    return x_encoded, encoder\n",
    "\n",
    "y_encoded, tle = label_encoder(y)\n",
    "\n",
    "y_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4]),\n",
       " array([12453, 72937, 18518, 15192, 10828], dtype=int64))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data spliting\n",
    "\n",
    "def data_spliter(X, y, test_size, random_state):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=seed)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = data_spliter(X_clean, y_encoded, test_size, seed)\n",
    "\n",
    "np.unique(y_train, return_counts=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4]), array([500, 500, 500, 500, 500], dtype=int64))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class balacing\n",
    "\n",
    "def random_under_sampler(X, y, max_num_samples_per_class,random_state=0):\n",
    "\n",
    "    labels, counts = np.unique(y, return_inverse=False, return_counts=True)\n",
    "    label_counts = dict(zip(range(len(labels)), [min(max_num_samples_per_class, count) for count in counts.tolist()]))\n",
    "    rus = RandomUnderSampler(sampling_strategy=label_counts, random_state=random_state)\n",
    "\n",
    "    try: \n",
    "        X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "        return X_resampled, y_resampled\n",
    "    except:\n",
    "        X_resampled, y_resampled = rus.fit_resample(X.reshape(-1, 1), y)\n",
    "        return X_resampled.reshape(-1, ), y_resampled\n",
    "\n",
    "X_train_resampled, y_train_resampled = random_under_sampler(X_train, y_train, max_num_samples_per_class)\n",
    "\n",
    "np.unique(y_train_resampled, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text classification pipeline: vectorizer + classifier\n",
    "\n",
    "pipe_steps = []\n",
    "\n",
    "vectorizer_parameters = {\n",
    "    \"stop_words\": \"english\", \n",
    "    \"ngram_range\": (1,1),\n",
    "    \"max_df\": 0.75, \n",
    "    \"min_df\": 0.00, \n",
    "    \"max_features\": 7000 ,\n",
    "    \"binary\": False, \n",
    "    \"use_idf\": True, \n",
    "    \"norm\": \"l2\",\n",
    "}\n",
    "vectorizer = TfidfVectorizer(**vectorizer_parameters)\n",
    "pipe_steps.append(('vec', vectorizer))\n",
    "\n",
    "classifier_parameters = {\n",
    "    'n_estimators': 15,\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.3,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    'colsample_bylevel': 1,\n",
    "    'colsample_bynode': 1,\n",
    "    'reg_lambda': 0.5,\n",
    "    'reg_alpha': 0,\n",
    "    'seed': seed\n",
    "}\n",
    "classifier = XGBClassifier(**classifier_parameters)\n",
    "pipe_steps.append(('clf', classifier))\n",
    "\n",
    "pipeline = Pipeline(pipe_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vec&#x27;,\n",
       "                 TfidfVectorizer(max_df=0.75, max_features=7000, min_df=0.0,\n",
       "                                 stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, device=None,\n",
       "                               early_stopping_rounds=None,\n",
       "                               enable_categorical=False, eval_metric=None,\n",
       "                               feature_types=None, gamma=None, grow_policy=None,\n",
       "                               importance_type=None,\n",
       "                               interaction_constraints=None, learning_rate=0.3,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=None,\n",
       "                               max_depth=5, max_leaves=None,\n",
       "                               min_child_weight=None, missing=nan,\n",
       "                               monotone_constraints=None, multi_strategy=None,\n",
       "                               n_estimators=15, n_jobs=None,\n",
       "                               num_parallel_tree=None,\n",
       "                               objective=&#x27;multi:softprob&#x27;, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vec&#x27;,\n",
       "                 TfidfVectorizer(max_df=0.75, max_features=7000, min_df=0.0,\n",
       "                                 stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, device=None,\n",
       "                               early_stopping_rounds=None,\n",
       "                               enable_categorical=False, eval_metric=None,\n",
       "                               feature_types=None, gamma=None, grow_policy=None,\n",
       "                               importance_type=None,\n",
       "                               interaction_constraints=None, learning_rate=0.3,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=None,\n",
       "                               max_depth=5, max_leaves=None,\n",
       "                               min_child_weight=None, missing=nan,\n",
       "                               monotone_constraints=None, multi_strategy=None,\n",
       "                               n_estimators=15, n_jobs=None,\n",
       "                               num_parallel_tree=None,\n",
       "                               objective=&#x27;multi:softprob&#x27;, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_df=0.75, max_features=7000, min_df=0.0,\n",
       "                stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              device=None, early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.3, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=15, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vec',\n",
       "                 TfidfVectorizer(max_df=0.75, max_features=7000, min_df=0.0,\n",
       "                                 stop_words='english')),\n",
       "                ('clf',\n",
       "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, device=None,\n",
       "                               early_stopping_rounds=None,\n",
       "                               enable_categorical=False, eval_metric=None,\n",
       "                               feature_types=None, gamma=None, grow_policy=None,\n",
       "                               importance_type=None,\n",
       "                               interaction_constraints=None, learning_rate=0.3,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=None,\n",
       "                               max_depth=5, max_leaves=None,\n",
       "                               min_child_weight=None, missing=nan,\n",
       "                               monotone_constraints=None, multi_strategy=None,\n",
       "                               n_estimators=15, n_jobs=None,\n",
       "                               num_parallel_tree=None,\n",
       "                               objective='multi:softprob', ...))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the text classification pipeline\n",
    "\n",
    "pipeline.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for train and test datasets\n",
    "\n",
    "y_train_resampled_pred = pipeline.predict(X_train_resampled)\n",
    "y_test_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metric report:\n",
      " {\n",
      "    \"accuracy\": 0.9172,\n",
      "    \"average_precision\": 0.9178,\n",
      "    \"average_recall\": 0.9172,\n",
      "    \"average_f1\": 0.9173\n",
      "}\n",
      "Test metric report:\n",
      " {\n",
      "    \"accuracy\": 0.7527,\n",
      "    \"average_precision\": 0.7885,\n",
      "    \"average_recall\": 0.7527,\n",
      "    \"average_f1\": 0.7606\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics\n",
    "\n",
    "def classifier_metrics_report_generator(y_true, y_pred, ndigits=4):\n",
    "  report = {}\n",
    "  report[\"accuracy\"] = round(accuracy_score(y_true, y_pred), ndigits)\n",
    "  report[\"average_precision\"] = round(precision_score(y_true, y_pred, average=\"weighted\"), ndigits)\n",
    "  report[\"average_recall\"] = round(recall_score(y_true, y_pred, average=\"weighted\"), ndigits)\n",
    "  report[\"average_f1\"] = round(f1_score(y_true, y_pred, average=\"weighted\"), ndigits)\n",
    "  return report\n",
    "\n",
    "metrics_report_train = classifier_metrics_report_generator(y_train_resampled_pred, y_train_resampled)\n",
    "metrics_report_test = classifier_metrics_report_generator(y_test, y_test_pred)\n",
    "\n",
    "print(\"Train metric report:\\n\", json.dumps(metrics_report_train, indent = 4))\n",
    "print(\"Test metric report:\\n\", json.dumps(metrics_report_test, indent = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'credit_card'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_i = [X_test[10]]\n",
    "y_i_encoded = pipeline.predict(x_i)\n",
    "y_i = tle.inverse_transform(y_i_encoded)\n",
    "y_i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y_pred': 'mortgages_and_loans', 'y_proba': 0.9571526}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a class to make predictions easier\n",
    "\n",
    "class TextClassPredictor:\n",
    "    \"\"\"Class to predict text category using trained models.\n",
    "\n",
    "    Args:\n",
    "        text_cleaner: function to clean the text.\n",
    "        target_encoder: pickle of the target label encoder.\n",
    "        classfier: pickle of the classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text_cleaner: Callable, target_encoder: pickle, classfier: pickle):\n",
    "        self.text_cleaner = text_cleaner\n",
    "        self.target_encoder = target_encoder\n",
    "        self.classfier = classfier\n",
    "\n",
    "    def predict(self, x: str):\n",
    "        \"\"\"Predict text category.\n",
    "\n",
    "        Args:\n",
    "            x: text to classify its category - narrative\n",
    "\n",
    "        Returns:\n",
    "            response: dictonary with predicted category and its probabiltiy.\n",
    "        \"\"\"\n",
    "        x_clean = self.text_cleaner([x])\n",
    "        y_encoded = self.classfier.predict(x_clean)\n",
    "        y_pred = self.target_encoder.inverse_transform(y_encoded)[0]\n",
    "        y_proba = max(self.classfier.predict_proba([x])[0])\n",
    "        response = {\"y_pred\": y_pred, \"y_proba\": y_proba}\n",
    "        return response\n",
    "\n",
    "\n",
    "tcp = TextClassPredictor(text_cleaner=text_cleaner, target_encoder=tle, classfier=pipeline)\n",
    "\n",
    "x_i = X_test[16]\n",
    "y_i = tcp.predict(x_i)\n",
    "y_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e404b6260610aa6bf8e80dd178743a20490f8e32854985c0e53b1c3b283487c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
